{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ef2eb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a3a0b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Factory = StemmerFactory()\n",
    "stemmer = Factory.create_stemmer()\n",
    "list_stopwords = set(stopwords.words('indonesian'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3ad0e9",
   "metadata": {},
   "source": [
    "## A.Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51c81b5",
   "metadata": {},
   "source": [
    "### A.1 Membaca 5 data teratas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbeb128a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>kalimat</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kalimat-kalimat tersebut adalah sebagai berikut</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Manusia membutuhkan makanan dan air supaya men...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tanaman hijau menggunakan air untuk membuat ma...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tanaman yang tidak mendapat air akan layu dan ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Keberhasilan belajar murid tidak hanya bergant...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12604</th>\n",
       "      <td>Saya suka menggunakan kendaraan ini untuk perj...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12605</th>\n",
       "      <td>Kendaraan ini memiliki desain interior yang me...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12606</th>\n",
       "      <td>Kendaraan tersebut sering mengalami masalah ke...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12607</th>\n",
       "      <td>Saya memilih kendaraan ini karena harganya kom...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12608</th>\n",
       "      <td>Kendaraan ini sangat nyaman untuk perjalanan j...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12609 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 kalimat  sentiment\n",
       "0        Kalimat-kalimat tersebut adalah sebagai berikut          0\n",
       "1      Manusia membutuhkan makanan dan air supaya men...          1\n",
       "2      Tanaman hijau menggunakan air untuk membuat ma...          0\n",
       "3      Tanaman yang tidak mendapat air akan layu dan ...          1\n",
       "4      Keberhasilan belajar murid tidak hanya bergant...          1\n",
       "...                                                  ...        ...\n",
       "12604  Saya suka menggunakan kendaraan ini untuk perj...          1\n",
       "12605  Kendaraan ini memiliki desain interior yang me...          1\n",
       "12606  Kendaraan tersebut sering mengalami masalah ke...          2\n",
       "12607  Saya memilih kendaraan ini karena harganya kom...          1\n",
       "12608  Kendaraan ini sangat nyaman untuk perjalanan j...          2\n",
       "\n",
       "[12609 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"Corpus Bahasa Indonesia Label.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7cc7af",
   "metadata": {},
   "source": [
    "### A.2 Cek data duplikat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6de74dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(1516)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcecf96b",
   "metadata": {},
   "source": [
    "### A.3 Melihat Dimensi Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfdd1652",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12609, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7a7217",
   "metadata": {},
   "source": [
    "### A.4 Cek Nilai Hilang / Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3991a830",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "kalimat      0\n",
       "sentiment    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa6c9e0",
   "metadata": {},
   "source": [
    "## B. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e8c9dd",
   "metadata": {},
   "source": [
    "### B.1 Handling Duplicate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ede09aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['kalimat', 'sentiment'], dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd0c5949",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(subset=['kalimat', 'sentiment'], keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2298a62",
   "metadata": {},
   "source": [
    "### B.2 Case Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af8e14aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['kalimat'] = df['kalimat'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755a968e",
   "metadata": {},
   "source": [
    "### B.3 Text Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7e40ef",
   "metadata": {},
   "source": [
    "1. Hapus URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32409b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['kalimat'] = df['kalimat'].str.replace(r'http\\S+|www\\S+|https\\S+', '', regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff910834",
   "metadata": {},
   "source": [
    "2. Hapus Tanda Baca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2eee7e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['kalimat'] = df['kalimat'].str.replace(r'[^a-zA-Z\\s]', '', regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b8e9b5",
   "metadata": {},
   "source": [
    "3. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a41980fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Tokenized_text'] = df['kalimat'].str.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a949c3",
   "metadata": {},
   "source": [
    "### B.5 Stopword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "17662639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Stopwords setelah Modifikasi 754\n"
     ]
    }
   ],
   "source": [
    "negation_words = {'tidak', 'bukan', 'tak'}\n",
    "\n",
    "list_stopwords = list_stopwords - negation_words\n",
    "print(f'Total Stopwords setelah Modifikasi {len(list_stopwords)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e20744fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      Tokenized_text  \\\n",
      "0  [kalimatkalimat, tersebut, adalah, sebagai, be...   \n",
      "1  [manusia, membutuhkan, makanan, dan, air, supa...   \n",
      "2  [tanaman, hijau, menggunakan, air, untuk, memb...   \n",
      "3  [tanaman, yang, tidak, mendapat, air, akan, la...   \n",
      "4  [keberhasilan, belajar, murid, tidak, hanya, b...   \n",
      "\n",
      "                               Stopword_Removed_Text  \n",
      "0                                   [kalimatkalimat]  \n",
      "1  [manusia, membutuhkan, makanan, air, kuat, seh...  \n",
      "2                  [tanaman, hijau, air, makanannya]  \n",
      "3                [tanaman, tidak, air, layu, kering]  \n",
      "4  [keberhasilan, belajar, murid, tidak, bergantu...  \n"
     ]
    }
   ],
   "source": [
    "def remove_sopwords(tokens):\n",
    " return[word for word in tokens if word not in list_stopwords]\n",
    "\n",
    "df['Stopword_Removed_Text'] = df['Tokenized_text'].apply(remove_sopwords)\n",
    "\n",
    "print(df[['Tokenized_text', 'Stopword_Removed_Text']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0786bd87",
   "metadata": {},
   "source": [
    "### B.6 Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b3ad175a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                   Stopword_Removed_Text  \\\n",
      "0                                       [kalimatkalimat]   \n",
      "1      [manusia, membutuhkan, makanan, air, kuat, seh...   \n",
      "2                      [tanaman, hijau, air, makanannya]   \n",
      "3                    [tanaman, tidak, air, layu, kering]   \n",
      "4      [keberhasilan, belajar, murid, tidak, bergantu...   \n",
      "...                                                  ...   \n",
      "12604                      [suka, kendaraan, perjalanan]   \n",
      "12605     [kendaraan, memiliki, desain, interior, mewah]   \n",
      "12606                [kendaraan, mengalami, kelistrikan]   \n",
      "12607         [memilih, kendaraan, harganya, kompetitif]   \n",
      "12608                    [kendaraan, nyaman, perjalanan]   \n",
      "\n",
      "                                         Clean_final  \n",
      "0                                     kalimatkalimat  \n",
      "1           manusia butuh makan air kuat sehat tanam  \n",
      "2                              tanam hijau air makan  \n",
      "3                        tanam tidak air layu kering  \n",
      "4      hasil ajar murid tidak gantung mutu guru pada  \n",
      "...                                              ...  \n",
      "12604                             suka kendara jalan  \n",
      "12605            kendara milik desain interior mewah  \n",
      "12606                          kendara alami listrik  \n",
      "12607                 pilih kendara harga kompetitif  \n",
      "12608                           kendara nyaman jalan  \n",
      "\n",
      "[11093 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "def stemming_text(tokens):\n",
    " # melakukan stemming pada setiap kata di dalam list\n",
    " stemmed_words = [stemmer.stem(word) for word in tokens]\n",
    "\n",
    " # gabungkan lagi menjadi string untuk Fetaure Extaction Selanjutnya\n",
    " return ' '.join(stemmed_words)\n",
    "\n",
    "df['Clean_final'] = df['Stopword_Removed_Text'].apply(stemming_text)\n",
    "\n",
    "print(df[['Stopword_Removed_Text', 'Clean_final']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fbc28c",
   "metadata": {},
   "source": [
    "## C.Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7ba4aa7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "0    5593\n",
       "2    2972\n",
       "1    2528\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4aeafee5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "0    50.419183\n",
       "2    26.791670\n",
       "1    22.789146\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sentiment'].value_counts(normalize=True) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1349cf",
   "metadata": {},
   "source": [
    "## D.Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0eccd6b",
   "metadata": {},
   "source": [
    "### D.1 Standarisasi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1f44ba",
   "metadata": {},
   "source": [
    "D.2 Split data stratfied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3c9d5363",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_raw = df['Clean_final']\n",
    "y_raw = df['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8e2f9249",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_split(x_raw, y_raw, train_size = 0.8, random_state = 42):\n",
    " np.random.seed(random_state)\n",
    "\n",
    " x_train_list, x_test_list = [], []\n",
    " y_train_list, y_test_list = [], []\n",
    "\n",
    " for class_value in np.unique(y_raw):\n",
    "  class_idx = np.where(y_raw == class_value)[0]\n",
    "\n",
    "  idx = np.random.permutation(class_idx)\n",
    "  split_ratio = int(len(idx) * train_size)\n",
    "\n",
    "  x_train_list.append(x_raw.iloc[idx[:split_ratio]])\n",
    "  x_test_list.append(x_raw.iloc[idx[split_ratio:]])\n",
    "  y_train_list.append(y_raw.iloc[idx[:split_ratio]])\n",
    "  y_test_list.append(y_raw.iloc[idx[split_ratio:]])\n",
    "\n",
    " x_train = pd.concat(x_train_list).reset_index(drop = True)\n",
    " x_test = pd.concat(x_test_list).reset_index(drop = True)\n",
    " y_train = pd.concat(y_train_list).reset_index(drop = True)\n",
    " y_test = pd.concat(y_test_list).reset_index(drop = True)\n",
    "\n",
    " return x_train, x_test, y_train, y_test\n",
    "\n",
    "x_train, x_test, y_train, y_test = stratified_split(x_raw, y_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "731d3fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 8873\n",
      "Test size: 2220\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train size: {len(x_train)}\")\n",
    "print(f\"Test size: {len(x_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "23fffccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensi x_train_features: (8873, 1000)\n"
     ]
    }
   ],
   "source": [
    "# --- A. PROSES DATA TRAIN (Membangun Rumus) ---\n",
    "# Input: x_train (dari hasil split sebelumnya)\n",
    "\n",
    "N_train = len(x_train)\n",
    "DF = {} \n",
    "vocabulary = set()\n",
    "Doc_TF_Train = [] \n",
    "\n",
    "# 1. Hitung TF dan DF hanya dari Train\n",
    "for doc in x_train:\n",
    "    tokens = doc.split()\n",
    "    doc_word_count = len(tokens)\n",
    "    \n",
    "    tf_doc = {}\n",
    "    for word in tokens:\n",
    "        tf_doc[word] = tf_doc.get(word, 0) + 1\n",
    "        vocabulary.add(word) # Catat kata unik\n",
    "    \n",
    "    # Normalisasi TF\n",
    "    for word, count in tf_doc.items():\n",
    "        tf_doc[word] = count / doc_word_count\n",
    "    \n",
    "    Doc_TF_Train.append(tf_doc)\n",
    "\n",
    "    # Update DF\n",
    "    for word in set(tokens):\n",
    "        DF[word] = DF.get(word, 0) + 1\n",
    "\n",
    "# 2. Urutkan Vocab & Buat Peta Index (Biar Cepat!)\n",
    "sorted_vocab_items = sorted(DF.items(), key=lambda item: item[1], reverse=True)\n",
    "LIMIT_FEATURES = 1000  # Kurangi parameter fitur di sini (bisa 500 atau 1000)\n",
    "\n",
    "final_vocab = [k for k, v in sorted_vocab_items[:LIMIT_FEATURES]] \n",
    "vocab_index = {word: i for i, word in enumerate(final_vocab)}\n",
    "\n",
    "# 3. Hitung IDF (Rumus dari Data Train)\n",
    "IDF = {}\n",
    "smoothing_factor = 1\n",
    "\n",
    "for word in final_vocab:\n",
    "    df_val = DF.get(word, 0) + smoothing_factor\n",
    "    IDF[word] = np.log10(N_train / df_val)\n",
    "\n",
    "# 4. Buat Matriks x_train_features\n",
    "x_train_features = np.zeros((N_train, len(final_vocab)))\n",
    "\n",
    "for i, tf_doc in enumerate(Doc_TF_Train):\n",
    "    for word, tf_val in tf_doc.items():\n",
    "        # Pakai vocab_index biar tidak looping manual (Jauh lebih cepat)\n",
    "        if word in vocab_index:\n",
    "            j = vocab_index[word] \n",
    "            tfidf_score = tf_val * IDF[word]\n",
    "            x_train_features[i, j] = tfidf_score\n",
    "\n",
    "print(f\"Dimensi x_train_features: {x_train_features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "063421d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensi x_test_features: (2220, 1000)\n"
     ]
    }
   ],
   "source": [
    "# --- B. PROSES DATA TEST (Hanya Mengubah Bentuk) ---\n",
    "# Input: x_test\n",
    "\n",
    "N_test = len(x_test)\n",
    "x_test_features = np.zeros((N_test, len(final_vocab)))\n",
    "\n",
    "for i, doc in enumerate(x_test):\n",
    "    tokens = doc.split()\n",
    "    doc_word_count = len(tokens)\n",
    "    \n",
    "    if doc_word_count == 0: continue \n",
    "\n",
    "    # Hitung TF Lokal untuk Test\n",
    "    tf_doc_test = {}\n",
    "    for word in tokens:\n",
    "        tf_doc_test[word] = tf_doc_test.get(word, 0) + 1\n",
    "    \n",
    "    # Masukkan ke Matriks\n",
    "    for word, count in tf_doc_test.items():\n",
    "        # Cek: Apakah kata ini ada di Vocab Train?\n",
    "        if word in vocab_index:\n",
    "            # Normalisasi TF\n",
    "            tf_val = count / doc_word_count\n",
    "            \n",
    "            # Ambil IDF dari Train (JANGAN hitung ulang!)\n",
    "            idf_val = IDF[word]\n",
    "            \n",
    "            # Ambil nomor kolom\n",
    "            j = vocab_index[word]\n",
    "            \n",
    "            # Masukkan nilai\n",
    "            x_test_features[i, j] = tf_val * idf_val\n",
    "        # Jika kata tidak ada di vocab_index, biarkan 0 (diabaikan)\n",
    "\n",
    "print(f\"Dimensi x_test_features: {x_test_features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dc3f17e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target jumlah sampel per kelas: 4474\n"
     ]
    }
   ],
   "source": [
    "# Pastikan y_label adalah numpy array\n",
    "y_train_array = y_train if isinstance(y_train, np.ndarray) else y_train.values\n",
    "\n",
    "# 1. Hitung jumlah sampel di kelas mayoritas (Target Oversampling)\n",
    "unique_classes, counts = np.unique(y_train_array, return_counts=True)\n",
    "max_count = counts.max() # Ini akan jadi target jumlah (5593)\n",
    "\n",
    "print(f\"Target jumlah sampel per kelas: {max_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "56824f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kelas 0: Sudah mayoritas (tidak ditambah).\n",
      "Kelas 1: Menambah 2452 sampel sintetik.\n",
      "Kelas 2: Menambah 2097 sampel sintetik.\n",
      "\n",
      "=== Oversampling Selesai ===\n",
      "Dimensi X_resampled: (13422, 1000)\n",
      "Dimensi y_resampled: (13422,)\n",
      "Distribusi Akhir: {np.int64(0): np.int64(4474), np.int64(1): np.int64(4474), np.int64(2): np.int64(4474)}\n"
     ]
    }
   ],
   "source": [
    "# List untuk menampung semua index (baris) yang akan kita ambil\n",
    "final_indices = []\n",
    "\n",
    "# 2. Iterasi setiap kelas untuk mengambil index\n",
    "for cls in unique_classes:\n",
    "    # Ambil index (nomor baris) yang memiliki label 'cls'\n",
    "    cls_indices = np.where(y_train_array == cls)[0]\n",
    "    n_current = len(cls_indices)\n",
    "    \n",
    "    # Masukkan index asli ke list final\n",
    "    final_indices.extend(cls_indices)\n",
    "    \n",
    "    # Hitung kekurangan sampel\n",
    "    n_needed = max_count - n_current\n",
    "    \n",
    "    if n_needed > 0:\n",
    "        print(f\"Kelas {cls}: Menambah {n_needed} sampel sintetik.\")\n",
    "        # Ambil index acak dari kelas tersebut (Resampling)\n",
    "        random_indices = np.random.choice(cls_indices, size=n_needed, replace=True)\n",
    "        final_indices.extend(random_indices)\n",
    "    else:\n",
    "        print(f\"Kelas {cls}: Sudah mayoritas (tidak ditambah).\")\n",
    "\n",
    "# 3. Buat Array Index Final dan Acak Urutannya (Shuffle)\n",
    "final_indices = np.array(final_indices)\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(final_indices)\n",
    "\n",
    "# 4. Ambil Data X dan y berdasarkan Index Final\n",
    "# Ini langsung memotong array X_features tanpa mengubahnya jadi DataFrame\n",
    "X_resampled = x_train_features[final_indices]\n",
    "y_resampled = y_train_array[final_indices]\n",
    "\n",
    "print(\"\\n=== Oversampling Selesai ===\")\n",
    "print(f\"Dimensi X_resampled: {X_resampled.shape}\")\n",
    "print(f\"Dimensi y_resampled: {y_resampled.shape}\")\n",
    "\n",
    "# Cek distribusi baru\n",
    "unique, counts = np.unique(y_resampled, return_counts=True)\n",
    "print(f\"Distribusi Akhir: {dict(zip(unique, counts))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "71d25867",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset = pd.DataFrame(X_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "daff3d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset['label'] = y_resampled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebf56a9",
   "metadata": {},
   "source": [
    "### 1. Gini Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b8a8ad32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hitung_gini(groups, classes):\n",
    " n_instance = float(sum(len(group) for group in groups))\n",
    " gini = 0.0\n",
    "\n",
    " for group in groups:\n",
    "  size = float(len(group))\n",
    "  if size == 0:\n",
    "   continue\n",
    "\n",
    "  score = 0.0\n",
    "  \n",
    "  y_group = group.iloc[:, -1]\n",
    "\n",
    "  for class_val in classes:\n",
    "   p = (y_group == class_val).sum() / size\n",
    "   score += p ** 2\n",
    "  gini += (1.0 - score) * (size / n_instance)\n",
    " return gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9fa8b765",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hitung_split(df_dataset, feat_idx, threshold):\n",
    " left = df_dataset[df_dataset.iloc[:,feat_idx] < threshold]\n",
    " right = df_dataset[df_dataset.iloc[:, feat_idx] >= threshold]\n",
    " return left, right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "22a63883",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_split(df_dataset, classes):\n",
    "    best_gini = float('inf')\n",
    "    best_split = {}\n",
    "    n_features = df_dataset.shape[1] - 1\n",
    "    \n",
    "    # Tips: Untuk percobaan cepat, batasi fitur (misal: range(min(100, n_features)))\n",
    "    # Jika ingin full, biarkan range(n_features)\n",
    "    for feat_idx in range(n_features):\n",
    "        \n",
    "        # --- PERBAIKAN: Pakai .unique() biar cepat ---\n",
    "        thresholds = df_dataset.iloc[:, feat_idx].unique()\n",
    "        \n",
    "        for threshold in thresholds:\n",
    "            groups = hitung_split(df_dataset, feat_idx, threshold)\n",
    "            gini = hitung_gini(groups, classes)\n",
    "            \n",
    "            if gini < best_gini:\n",
    "                best_gini = gini\n",
    "                best_split = {\n",
    "                    'feat_idx' : feat_idx,\n",
    "                    'val' : threshold,\n",
    "                    'groups' : groups\n",
    "                }\n",
    "    return best_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a61a15c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Fungsi untuk menentukan hasil akhir (Label) jika sudah di ujung pohon (daun)\n",
    "def to_terminal(group):\n",
    "    # Mengambil modus (label yang paling sering muncul) di kolom terakhir\n",
    "    return group.iloc[:, -1].mode()[0]\n",
    "\n",
    "# 2. Fungsi Rekursif Membangun Pohon\n",
    "def build_tree(df_dataset, classes, depth, max_depth):\n",
    "    # Cari split terbaik untuk node saat ini\n",
    "    root = get_best_split(df_dataset, classes)\n",
    "    \n",
    "    # Jika tidak ditemukan split (misal data sudah murni), jadikan terminal node\n",
    "    if not root:\n",
    "        return to_terminal(df_dataset) \n",
    "    \n",
    "    # Ambil hasil split\n",
    "    left, right = root['groups']\n",
    "    del(root['groups']) # Hapus data dari dictionary agar hemat memori\n",
    "    \n",
    "    # Jika salah satu sisi kosong, berarti tidak bisa di-split lagi -> buat terminal\n",
    "    if len(left) == 0 or len(right) == 0:\n",
    "        no_split_data = pd.concat([left, right])\n",
    "        root['left'] = root['right'] = to_terminal(no_split_data)\n",
    "        return root\n",
    "    \n",
    "    # Jika sudah mencapai kedalaman maksimum, stop dan buat terminal\n",
    "    if depth >= max_depth:\n",
    "        root['left'] = to_terminal(left)\n",
    "        root['right'] = to_terminal(right)\n",
    "        return root\n",
    "    \n",
    "    # Jika belum stop, panggil lagi fungsi ini (rekursif) untuk anak kiri dan kanan\n",
    "    root['left'] = build_tree(left, classes, depth + 1, max_depth)\n",
    "    root['right'] = build_tree(right, classes, depth + 1, max_depth)\n",
    "    \n",
    "    return root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "27978745",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_row(node, row):\n",
    "    if not isinstance(node, dict):\n",
    "        return node\n",
    "    \n",
    "    if row.iloc[node['feat_idx']] < node['val']:\n",
    "        return predict_row(node['left'], row)\n",
    "    else:\n",
    "        return predict_row(node['right'], row)\n",
    "    \n",
    "def predict_batch(tree, df_test):\n",
    "    predictions = []\n",
    "    for index, row in df_test.iterrows():\n",
    "        prediction = predict_row(tree, row)\n",
    "        predictions.append(prediction)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "07022566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Training (Gunakan data hasil oversampling & TF-IDF yang sudah kamu buat sebelumnya)\n",
    "# df_dataset ini sudah berisi fitur X_resampled dan kolom 'label' di akhir\n",
    "my_tree = build_tree(df_dataset, classes=[0, 1, 2], depth=0, max_depth=5)\n",
    "\n",
    "# 2. Testing (Siapkan data test agar formatnya sama dengan data train)\n",
    "# Kamu harus menjadikan x_test_features sebagai DataFrame dulu\n",
    "test_data = pd.DataFrame(x_test_features)\n",
    "# (Opsional) Tambahkan label jika ingin mengecek akurasi nanti, tapi untuk prediksi tidak wajib\n",
    "# test_data['label'] = y_test.values \n",
    "\n",
    "# 3. Prediksi\n",
    "predictions_test = predict_batch(my_tree, test_data)\n",
    "predictions_train = predict_batch(my_tree, df_dataset) # Prediksi ke data train sendiri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694efdae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
