{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "3c0a70e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nahls\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\nahls\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\nahls\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kategori sebelum dibersihkan: ['ham' 'spam' '{\"mode\":\"full\"']\n",
      "Jumlah data bersih: 5572\n",
      "Sedang memproses NLTK (English)...\n",
      "                                             Message  \\\n",
      "0  Go until jurong point, crazy.. Available only ...   \n",
      "1                      Ok lar... Joking wif u oni...   \n",
      "2  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
      "3  U dun say so early hor... U c already then say...   \n",
      "4  Nah I don't think he goes to usf, he lives aro...   \n",
      "\n",
      "                                       clean_message  \n",
      "0  go jurong point crazy available bugis great wo...  \n",
      "1                              ok lar joking wif oni  \n",
      "2  free entry wkly comp win fa cup final tkts st ...  \n",
      "3                  dun say early hor see already say  \n",
      "4                nah think go usf life around though  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "# --- 1. SETUP LIBRARY (ENGLISH VERSION) ---\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet') # Untuk Lemmatization\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Load Stopwords Inggris\n",
    "list_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "# Pilih salah satu: Stemmer (Potong imbuhan) atau Lemmatizer (Cari kata dasar baku)\n",
    "# Rekomendasi: Lemmatizer lebih bagus untuk analisis sentimen/konteks\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "# stemmer = PorterStemmer() # Opsi alternatif kalau mau cepat & kasar\n",
    "\n",
    "# Kamus Singkatan (Bahasa Inggris banyak singkatan chat)\n",
    "kamus_singkatan = {\n",
    "    \"u\": \"you\",\n",
    "    \"ur\": \"your\",\n",
    "    \"r\": \"are\",\n",
    "    \"n\": \"and\",\n",
    "    \"d\": \"the\",\n",
    "    \"b4\": \"before\",\n",
    "    \"c\": \"see\",\n",
    "    \"lol\": \"laughing out loud\",\n",
    "    \"txt\": \"text\",\n",
    "    \"2\": \"to\",\n",
    "    \"4\": \"for\",\n",
    "    # Tambahkan lagi sesuai temuan di data\n",
    "}\n",
    "\n",
    "# --- 2. FUNGSI LINGUISTIC PROCESSING ---\n",
    "def linguistic_processing_english(list_of_words):\n",
    "    clean_words = []\n",
    "    \n",
    "    for word in list_of_words:\n",
    "        # A. Normalisasi Singkatan (Chat/Alay Inggris)\n",
    "        word = kamus_singkatan.get(word, word)\n",
    "        \n",
    "        # B. Stopword Removal\n",
    "        if word not in list_stopwords:\n",
    "            # C. Lemmatization (Lebih bagus dari Stemming untuk Inggris)\n",
    "            # Contoh: \"better\" -> \"good\", \"running\" -> \"run\"\n",
    "            lemma_word = lemmatizer.lemmatize(word)\n",
    "            clean_words.append(lemma_word)\n",
    "            \n",
    "    return \" \".join(clean_words)\n",
    "\n",
    "# --- 3. EKSEKUSI PADA DATAFRAME ---\n",
    "\n",
    "# Load Data\n",
    "df = pd.read_csv('email.csv')\n",
    "\n",
    "# --- TAMBAHAN WAJIB: BERSIHKAN DATA SAMPAH ---\n",
    "# Cek apakah ada label aneh\n",
    "print(\"Kategori sebelum dibersihkan:\", df['Category'].unique())\n",
    "\n",
    "# Lakukan Mapping\n",
    "df['label_enc'] = df['Category'].map({'ham': 0, 'spam': 1})\n",
    "\n",
    "# Hapus baris yang labelnya NaN (Sampah tadi)\n",
    "df.dropna(subset=['label_enc'], inplace=True)\n",
    "\n",
    "# Pastikan tipe data integer\n",
    "df['label_enc'] = df['label_enc'].astype(int)\n",
    "\n",
    "print(\"Jumlah data bersih:\", len(df))\n",
    "\n",
    "# TAHAP 1: Cleaning Cepat (Pandas)\n",
    "# Hapus angka, tanda baca, lower, dan split\n",
    "df['temp_tokens'] = df['Message'].str.lower() \\\n",
    "                                 .str.replace(r'[^a-z\\s]', ' ', regex=True) \\\n",
    "                                 .str.split()\n",
    "\n",
    "# TAHAP 2: Linguistic Processing (NLTK)\n",
    "print(\"Sedang memproses NLTK (English)...\")\n",
    "df['clean_message'] = df['temp_tokens'].apply(linguistic_processing_english)\n",
    "\n",
    "# Hapus kolom bantuan\n",
    "df.drop(columns=['temp_tokens'], inplace=True)\n",
    "\n",
    "# Cek Hasil\n",
    "print(df[['Message', 'clean_message']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "790f627d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah Awal -> Ham: 4825, Spam: 747\n",
      "Jumlah Setelah Oversampling -> Ham: 4825, Spam: 4825\n"
     ]
    }
   ],
   "source": [
    "# 1. Pisahkan Data Ham dan Spam\n",
    "df_ham = df[df['label_enc'] == 0]\n",
    "df_spam = df[df['label_enc'] == 1]\n",
    "\n",
    "print(f\"Jumlah Awal -> Ham: {len(df_ham)}, Spam: {len(df_spam)}\")\n",
    "\n",
    "# 2. Oversample Spam (Duplikasi data spam agar sama banyak dengan Ham)\n",
    "# Kita ambil sampel spam secara acak berulang-ulang sampai jumlahnya sama dengan Ham\n",
    "df_spam_oversampled = df_spam.sample(n=len(df_ham), replace=True, random_state=42)\n",
    "\n",
    "# 3. Gabungkan Kembali\n",
    "df_balanced = pd.concat([df_ham, df_spam_oversampled])\n",
    "\n",
    "# 4. Acak (Shuffle) agar posisi Ham dan Spam tercampur\n",
    "df_balanced = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(f\"Jumlah Setelah Oversampling -> Ham: {len(df_balanced[df_balanced['label_enc']==0])}, Spam: {len(df_balanced[df_balanced['label_enc']==1])}\")\n",
    "\n",
    "# 5. Gunakan df_balanced untuk Split Data selanjutnya\n",
    "split_index = int(len(df_balanced) * 0.8)\n",
    "df_train = df_balanced.iloc[:split_index]\n",
    "df_test = df_balanced.iloc[split_index:]\n",
    "\n",
    "# ... (Lanjut ke TF-IDF dan Training seperti biasa) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "fe8d73c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_transform_tfidf(text_series):\n",
    "    # --- A. BANGUN VOCABULARY (Kamus Kata) ---\n",
    "    # Ambil semua kata unik di data training\n",
    "    print(\"Membangun Vocabulary...\")\n",
    "    all_words = \" \".join(text_series).split()\n",
    "    vocab = sorted(list(set(all_words)))\n",
    "    \n",
    "    # Mapping kata ke index biar cepat (word: index)\n",
    "    vocab_index = {word: i for i, word in enumerate(vocab)}\n",
    "    n_docs = len(text_series)\n",
    "    n_vocab = len(vocab)\n",
    "    \n",
    "    # --- B. HITUNG DF (Document Frequency) ---\n",
    "    # Berapa banyak dokumen yang mengandung kata X?\n",
    "    print(\"Menghitung IDF...\")\n",
    "    df_counts = dict.fromkeys(vocab, 0)\n",
    "    \n",
    "    for text in text_series:\n",
    "        # Set(text.split()) biar kata yang muncul 2x dalam 1 SMS tetap dihitung 1\n",
    "        unique_words_in_doc = set(text.split())\n",
    "        for word in unique_words_in_doc:\n",
    "            if word in df_counts:\n",
    "                df_counts[word] += 1\n",
    "                \n",
    "    # --- C. HITUNG IDF (Inverse Document Frequency) ---\n",
    "    # Rumus: log(Total Dokumen / (Jumlah Dokumen yg ada kata itu + 1))\n",
    "    idf_values = {}\n",
    "    for word, count in df_counts.items():\n",
    "        # +1 di penyebut untuk smoothing (biar ga error bagi 0)\n",
    "        idf_values[word] = np.log(n_docs / (count + 1))\n",
    "        \n",
    "    # --- D. HITUNG TF-IDF (TF * IDF) ---\n",
    "    print(\"Menghitung TF-IDF Matrix...\")\n",
    "    # Kita pakai Numpy Array kosong dulu\n",
    "    tfidf_matrix = np.zeros((n_docs, n_vocab))\n",
    "    \n",
    "    for row, text in enumerate(text_series):\n",
    "        words = text.split()\n",
    "        if len(words) == 0: continue\n",
    "            \n",
    "        # Hitung TF (Frekuensi kata di SMS ini)\n",
    "        word_counts = {}\n",
    "        for w in words:\n",
    "            word_counts[w] = word_counts.get(w, 0) + 1\n",
    "            \n",
    "        # Isi Matrix\n",
    "        for word, count in word_counts.items():\n",
    "            if word in vocab_index:\n",
    "                # Rumus TF Biasa: Jumlah Kemunculan\n",
    "                tf = count \n",
    "                # (Opsional: Bisa pakai tf = count / len(words) untuk normalisasi)\n",
    "                \n",
    "                col_idx = vocab_index[word]\n",
    "                tfidf_matrix[row, col_idx] = tf * idf_values[word]\n",
    "                \n",
    "    return tfidf_matrix, vocab, vocab_index, idf_values\n",
    "\n",
    "def transform_tfidf(text_series, vocab_index, idf_values):\n",
    "    # Fungsi ini untuk DATA TEST. Tidak belajar vocab baru, cuma pakai yang ada.\n",
    "    n_docs = len(text_series)\n",
    "    n_vocab = len(vocab_index)\n",
    "    tfidf_matrix = np.zeros((n_docs, n_vocab))\n",
    "    \n",
    "    for row, text in enumerate(text_series):\n",
    "        words = text.split()\n",
    "        word_counts = {}\n",
    "        for w in words:\n",
    "            word_counts[w] = word_counts.get(w, 0) + 1\n",
    "            \n",
    "        for word, count in word_counts.items():\n",
    "            # Cuma hitung kata yang ada di Vocabulary Training\n",
    "            if word in vocab_index:\n",
    "                tf = count\n",
    "                col_idx = vocab_index[word]\n",
    "                tfidf_matrix[row, col_idx] = tf * idf_values[word]\n",
    "                \n",
    "    return tfidf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "b34cc1f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Membangun Vocabulary...\n",
      "Menghitung IDF...\n",
      "Menghitung TF-IDF Matrix...\n",
      "Selesai!\n",
      "Ukuran Matrix Train: (7720, 6466)\n",
      "Ukuran Matrix Test : (1930, 6466)\n"
     ]
    }
   ],
   "source": [
    "# 1. Latih pada Data Train\n",
    "X_train_matrix, vocab, vocab_index, idf_model = fit_transform_tfidf(df_train['clean_message'])\n",
    "\n",
    "# 2. Terapkan pada Data Test (Jangan intip data test!)\n",
    "X_test_matrix = transform_tfidf(df_test['clean_message'], vocab_index, idf_model)\n",
    "\n",
    "# 3. Siapkan Label (y)\n",
    "y_train = df_train['label_enc'].values\n",
    "y_test = df_test['label_enc'].values\n",
    "\n",
    "print(\"Selesai!\")\n",
    "print(f\"Ukuran Matrix Train: {X_train_matrix.shape}\")\n",
    "print(f\"Ukuran Matrix Test : {X_test_matrix.shape}\")\n",
    "# Harusnya (Jumlah Data, Jumlah Kata Unik)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "7ef023c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melakukan Normalisasi L2...\n",
      "Nilai max sekarang: 1.0\n"
     ]
    }
   ],
   "source": [
    "# --- Tambahkan Fungsi Ini ---\n",
    "def normalize_l2(matrix):\n",
    "    # Hitung akar kuadrat dari jumlah kuadrat setiap baris (Magnitude)\n",
    "    norms = np.sqrt(np.sum(matrix**2, axis=1, keepdims=True))\n",
    "    \n",
    "    # Hindari pembagian dengan 0 (kalau ada baris kosong)\n",
    "    norms[norms == 0] = 1\n",
    "    \n",
    "    # Bagi setiap nilai dengan magnitude-nya agar range-nya 0-1\n",
    "    return matrix / norms\n",
    "\n",
    "# --- Panggil Fungsi Ini Sebelum Training ---\n",
    "# (Lakukan setelah cell TF-IDF selesai)\n",
    "\n",
    "print(\"Melakukan Normalisasi L2...\")\n",
    "X_train_matrix = normalize_l2(X_train_matrix)\n",
    "X_test_matrix = normalize_l2(X_test_matrix)\n",
    "\n",
    "print(\"Nilai max sekarang:\", np.max(X_train_matrix)) # Harusnya maks 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "235df281",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_equation(X, w, b):\n",
    "    # X: Data fitur\n",
    "    # w: Bobot (weights)\n",
    "    # b: Bias\n",
    "    z = np.dot(X, w) + b\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "a8e00342",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    # z: Hasil dari linear equation\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "1bad1c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_loss(y_true, y_pred):\n",
    "    # y_true: Kunci jawaban asli (0 atau 1)\n",
    "    # y_pred: Prediksi probabilitas dari sigmoid\n",
    "    \n",
    "    epsilon = 1e-15\n",
    "    # Jepit nilai biar gak 0 murni atau 1 murni\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    loss = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "6e8972b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_desc(X, y_true, y_pred):\n",
    "    m = X.shape[0] # Jumlah data\n",
    "    \n",
    "    # Hitung selisih tebakan (Error term)\n",
    "    error = y_pred - y_true\n",
    "    \n",
    "    # Turunan terhadap bobot (w)\n",
    "    dw = (1 / m) * np.dot(X.T, error)\n",
    "    \n",
    "    # Turunan terhadap bias (b)\n",
    "    db = (1 / m) * np.sum(error)\n",
    "    \n",
    "    return dw, db   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "22b0ead0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(w, b, dw, db, learning_rate):\n",
    "    \n",
    "    w_new = w - learning_rate * dw\n",
    "    b_new = b - learning_rate * db\n",
    "    \n",
    "    return w_new, b_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "8abaa445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mulai Training:\n",
      "Epoch 0: Loss = 0.6931\n",
      "Epoch 50: Loss = 0.3191\n",
      "Epoch 100: Loss = 0.2298\n",
      "Epoch 150: Loss = 0.1851\n",
      "Epoch 200: Loss = 0.1571\n",
      "Epoch 250: Loss = 0.1374\n",
      "Epoch 300: Loss = 0.1226\n",
      "Epoch 350: Loss = 0.1111\n",
      "Epoch 400: Loss = 0.1017\n",
      "Epoch 450: Loss = 0.0940\n",
      "Epoch 500: Loss = 0.0874\n",
      "Epoch 550: Loss = 0.0818\n",
      "Epoch 600: Loss = 0.0769\n",
      "Epoch 650: Loss = 0.0727\n",
      "Epoch 700: Loss = 0.0689\n",
      "Epoch 750: Loss = 0.0655\n",
      "Epoch 800: Loss = 0.0624\n",
      "Epoch 850: Loss = 0.0597\n",
      "Epoch 900: Loss = 0.0572\n",
      "Epoch 950: Loss = 0.0549\n",
      "Epoch 1000: Loss = 0.0528\n",
      "Training Selesai\n"
     ]
    }
   ],
   "source": [
    "# 1. Inisialisasi Parameter Awal (Semuanya nol)\n",
    "# Kita butuh w sebanyak jumlah kolom fitur (X_train.shape[1])\n",
    "w = np.zeros(X_train_matrix.shape[1])\n",
    "b = 0\n",
    "learning_rate = 10\n",
    "epochs = 1001 # Jumlah putaran belajar\n",
    "loss_history = []\n",
    "\n",
    "# 2. Loop Belajar\n",
    "print(\"Mulai Training:\")\n",
    "for i in range(epochs):\n",
    "    # A. Maju (Forward Pass)\n",
    "    z = linear_equation(X_train_matrix, w, b)       # Rumus 1\n",
    "    y_pred = sigmoid(z)                      # Rumus 2\n",
    "    \n",
    "    # B. Hitung Error (Opsional, buat grafik)\n",
    "    loss = log_loss(y_train, y_pred)     # Rumus 3\n",
    "    loss_history.append(loss)\n",
    "    \n",
    "    # C. Mundur (Backward Pass - Cari arah perbaikan)\n",
    "    dw, db = grad_desc(X_train_matrix, y_train, y_pred) # Rumus 4\n",
    "    \n",
    "    # D. Update (Perbaiki bobot)\n",
    "    w, b = update_parameters(w, b, dw, db, learning_rate) # Rumus 5\n",
    "    \n",
    "    # Print progress setiap 100 putaran\n",
    "    if i % 50 == 0:\n",
    "        print(f\"Epoch {i}: Loss = {loss:.4f}\")\n",
    "\n",
    "print(\"Training Selesai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "7215e81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, w, b, threshold=0.6):\n",
    "    # 1. Hitung probabilitas (sama kayak waktu training)\n",
    "    z = linear_equation(X, w, b)\n",
    "    y_prob = sigmoid(z)\n",
    "    \n",
    "    # 2. Ubah probabilitas jadi kelas (0 atau 1)\n",
    "    # List comprehension: Kalau p > 0.5 jadi 1, selain itu 0\n",
    "    y_class = [1 if p > threshold else 0 for p in y_prob]\n",
    "    \n",
    "    return np.array(y_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "b7aeee92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.99\n"
     ]
    }
   ],
   "source": [
    "predictions = predict(X_test_matrix, w, b)\n",
    "akurasi_test = np.sum(predictions == y_test) / len(X_test_matrix)\n",
    "print(f\"{akurasi_test:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "d4d03af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.99\n"
     ]
    }
   ],
   "source": [
    "prediction_train = predict(X_train_matrix, w, b)\n",
    "akurasi_train = np.sum(prediction_train == y_train) / len(X_train_matrix)\n",
    "print(f\"{akurasi_train:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "ffef7725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positive: 944\n",
      "False Positive: 8\n",
      "True Negative: 965\n",
      "False Negative: 13\n"
     ]
    }
   ],
   "source": [
    "TP = np.sum((predictions == 1) & (y_test == 1))\n",
    "FP = np.sum((predictions == 1) & (y_test == 0))\n",
    "TN = np.sum((predictions == 0) & (y_test == 0))\n",
    "FN = np.sum((predictions == 0) & (y_test == 1))\n",
    "\n",
    "print(f\"True Positive: {TP}\")\n",
    "print(f\"False Positive: {FP}\")\n",
    "print(f\"True Negative: {TN}\")\n",
    "print(f\"False Negative: {FN}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "1142a3fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.99\n"
     ]
    }
   ],
   "source": [
    "precision = TP / (TP + FP)\n",
    "print(f\"{precision:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
